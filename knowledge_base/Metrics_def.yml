# ===================================================================================
# AI Knowledge Base for the Client Experience (CX) dbt Project
# Version: 3.0
# Last Updated: 2025-09-19
# Description: This document provides a comprehensive overview of the dbt models
#              used for analyzing customer interaction metrics. It is intended to be
#              used as a knowledge source for AI assistants and developers. It contains
#              both technical model architecture and a detailed business metrics catalog.
# ===================================================================================

dbt_project_knowledge_base:
  
  # ---------------------------------------------------------------------------------
  # Architectural Principles
  # ---------------------------------------------------------------------------------
  architecture:
    philosophy: "Interaction-First"
    description: >
      The entire analytics layer is built on an interaction-level foundation. The most granular
      unit of analysis is a single, continuous segment of work done by one agent (an 'interaction').
      This provides maximum flexibility. Metrics that are naturally conversation-level (like First
      Response Time) are calculated once and then joined down to each interaction, providing
      full context on every row.
    data_unification_strategy: >
      The project unifies data from two distinct sources: 'Phone' (from Five9) and 'Chat'
      (from Decagon). The `prep_interaction_times` model is responsible for this unification,
      casting all identifiers to VARCHAR and numeric metrics to FLOAT to ensure compatibility
      and prevent data type errors during UNION operations.

  # ---------------------------------------------------------------------------------
  # Model Definitions
  # ---------------------------------------------------------------------------------
  models:
    - name: "prep_interaction_times"
      path: "models/prep/prep_interaction_times.sql"
      purpose: >
        This is the most critical foundational model. It unifies raw phone and chat data into a single,
        standardized structure. It is responsible for calculating granular, interaction-specific
        timestamps, durations (handle time, hold time), and flags (is_transfer_interaction).
      grain: "One row per unique interaction (_interaction_id)."
      materialization: "view"
      dependencies:
        - "prep_phone_segments"
        - "prep_chat_messages"

    - name: "prep_phone_metrics"
      path: "models/prep/prep_phone_metrics.sql"
      purpose: >
        Calculates metrics that are defined once for the entire lifecycle of a phone conversation.
        Primarily, it calculates First Response Time (FRT) for the phone channel.
      grain: "One row per unique phone conversation (five9_call_id)."
      materialization: "table"
      dependencies:
        - "prep_phone_segments"

    - name: "prep_chat_metrics"
      path: "models/prep/prep_chat_metrics.sql"
      purpose: >
        Calculates metrics that are defined once for the entire lifecycle of a chat conversation.
        It calculates both bot FRT and agent FRT for the chat channel.
      grain: "One row per unique chat conversation (decagon_conversation_id)."
      materialization: "table"
      dependencies:
        - "prep_chat_messages"

    - name: "fct_interactions"
      path: "models/marts/fct_interactions.sql"
      purpose: >
        This is the primary fact model for consumption by BI tools or analysis. It joins the foundational
        `prep_interaction_times` model with the conversation-level metrics from the phone and chat
        prep models. The result is a comprehensive table where every row represents a single interaction
        but also contains the full context of the conversation it belongs to.
      grain: "One row per unique interaction (_interaction_id)."
      materialization: "table"
      dependencies:
        - "prep_interaction_times"
        - "prep_phone_metrics"
        - "prep_chat_metrics"
    
    - name: "fct_interactions_containment"
      path: "models/marts/fct_interactions_containment.sql"
      purpose: >
        A specialized fact table focused on customer interaction containment. It includes only interactions
        where the customer's identity is known, which is necessary for tracking re-reach behavior. This model
        is the source for key containment KPIs.
      grain: "One row per unique interaction (_interaction_id)."
      materialization: "table"
      dependencies:
        - "prep_interaction_times"
        - "some_identity_mapping_model" # Assumed dependency for identity_canonical_id

    - name: "mart_cxo_interactions_containment"
      path: "models/marts/mart_cxo_interactions_containment.sql"
      purpose: >
        A high-level, enriched data mart designed for executive (CXO) reporting. It builds on the containment
        fact table by adding business context such as topic modeling (ERNIE), agent level, and routing skill.
      grain: "One row per unique interaction (_interaction_id)."
      materialization: "table"
      dependencies:
        - "fct_interactions_containment"
        - "some_topic_modeling_model" # Assumed dependency for ERNIE topics
        - "dim_agents" # Assumed dependency for l1_agent_flg

  # ---------------------------------------------------------------------------------
  # Key Column and Metric Dictionary (From dbt models)
  # ---------------------------------------------------------------------------------
  metrics_and_columns:
    - name: "_interaction_id"
      description: "A unique identifier for a single, continuous interaction handled by one agent. Serves as the surrogate key for the main fact tables."
      granularity: "Interaction-Level"
      calculation_summary: "Derived directly from source systems and cast to VARCHAR for unification."

    - name: "conversation_id"
      description: "A unique identifier for an entire customer conversation, which may contain one or more interactions."
      granularity: "Conversation-Level"
      calculation_summary: "Derived from `five9_call_id` for phone or `decagon_conversation_id` for chat."
    
    - name: "identity_canonical_id"
      description: "The unique, canonical identifier for the client in Wealthsimple's systems. This is crucial for tracking a customer's journey across multiple conversations."
      granularity: "Customer-Level"
      calculation_summary: "Joined in from an identity resolution or customer dimension model."

    - name: "interaction_handle_time_seconds"
      description: "The active time, in seconds, that an agent spent working on this specific interaction. For phone, this is the segment duration. For chat, it's the time from the first agent message to the last."
      granularity: "Interaction-Level"
      calculation_summary: "Calculated as `interaction_ended_at` - `interaction_started_at` in `prep_interaction_times`."

    - name: "is_transfer_interaction"
      description: "A binary flag (1 or 0) indicating if this interaction was the result of a transfer. An interaction is considered a transfer if it is not the first interaction in the conversation AND the agent is different from the agent who handled the immediately preceding interaction."
      granularity: "Interaction-Level"
      calculation_summary: "Uses a LAG() window function in `prep_interaction_times` to compare the current agent with the previous agent."
    
    - name: "interaction_handoff_time_seconds"
      description: "The time, in seconds, between the end of the previous interaction and the start of the current one. This represents queue time or transfer delay. It is NULL for the first interaction in a conversation."
      granularity: "Interaction-Level"
      calculation_summary: "Uses a LAG() window function in `prep_interaction_times` to calculate the DATEDIFF between the previous `interaction_ended_at` and the current `interaction_started_at`."

    - name: "first_response_time_seconds"
      description: "The total time, in seconds, from when a customer initiated the conversation (first call or message) until the first human agent responded."
      granularity: "Conversation-Level"
      calculation_summary: "Calculated in `prep_phone_metrics` and `prep_chat_metrics` by finding the MIN customer timestamp and the MIN agent timestamp for the entire conversation."

    # --- Containment Metrics ---
    - name: "deflected_flg"
      description: "A boolean flag indicating if the interaction was successfully deflected. A deflection occurs when the customer's issue is resolved at the first point of contact without needing to be escalated to another human agent."
      granularity: "Interaction-Level"
      calculation_summary: "A business logic rule is applied, likely based on whether the interaction is the only one in the conversation or if it was handled entirely by a bot."

    - name: "rereach_24h_flg"
      description: "A boolean flag indicating if the same customer (`identity_canonical_id`) initiated a new interaction within 24 hours of this interaction ending. This is a key indicator that the initial resolution was not successful."
      granularity: "Interaction-Level"
      calculation_summary: "Requires a lookahead function or self-join to check for subsequent interactions from the same `identity_canonical_id` within a 24-hour window."

    - name: "contained_flg"
      description: "The primary success metric for deflection. It is a boolean flag that is TRUE only if an interaction was successfully deflected (`deflected_flg` is TRUE) AND the customer did not re-reach within 24 hours (`rereach_24h_flg` is FALSE)."
      granularity: "Interaction-Level"
      calculation_summary: "Calculated as: `CASE WHEN deflected_flg = TRUE AND rereach_24h_flg = FALSE THEN TRUE ELSE FALSE END`."

    - name: "containment_type"
      description: "A detailed categorization of the interaction's outcome, providing a full picture of the customer journey."
      granularity: "Interaction-Level"
      calculation_summary: "A CASE statement is applied based on the values of `deflected_flg`, `rereach_24h_flg`, and `is_transfer_interaction`. Possible values include: 'contained', 'rereach_in_24h', 'escalation_to_agent', 'transfer_to_agent', 'rereach_and_escalation'."

    # --- Enriched Mart Columns ---
    - name: "routing_topic"
      description: "The topic or skill that the interaction was initially routed to (e.g., 'Decagon topic' for chat, 'five9skill' for phone)."
      granularity: "Interaction-Level"
      calculation_summary: "Derived from source system data."

    - name: "parent_topic"
      description: "The highest-level topic category assigned by the ERNIE topic modeling engine, based on analysis of the interaction's content."
      granularity: "Interaction-Level"
      calculation_summary: "Joined from a topic modeling results table."

    - name: "topic"
      description: "The main topic assigned by the ERNIE topic modeling engine."
      granularity: "Interaction-Level"
      calculation_summary: "Joined from a topic modeling results table."

    - name: "subtopic"
      description: "The most specific subtopic assigned by the ERNIE topic modeling engine."
      granularity: "Interaction-Level"
      calculation_summary: "Joined from a topic modeling results table."

    - name: "l1_agent_flg"
      description: "A flag (1 or 0) indicating if the handling agent is a Level 1 (BPO) agent (1) or an internal agent (0)."
      granularity: "Agent-Level"
      calculation_summary: "Joined from an agent dimension table (`dim_agents`)."

  # ===================================================================================
  # COMPREHENSIVE BUSINESS METRICS DEFINITION CATALOG
  # ===================================================================================
  business_metrics_catalog:
    metrics:
      # =============================================================================
      # CUSTOMER SUPPORT (CS) DOMAIN METRICS
      # =============================================================================
      - name: adherence
        label: Adherence
        model: ref('dim_agent_scheduling_daily')
        description: >
          Amount of time spent in adherent state relative to productive time. Used to measure agent schedule compliance.
        type: ratio
        sql: sum(1.0000 * adherence_productive_time) / nullif({{ metric('scheduled_productive_hours') }} * 3600, 0)
        timestamp: summary_date
        time_grains: [day, week, month, quarter, year]
        dimensions:
          - agent
          - team
        meta:
          domain: cs
          category: agent_performance
          definition_long: >
            Adherence is a performance metric that quantifies the degree to which a customer support agent complies with their assigned schedule. It is calculated as the total time an agent spent in a value-adding, "adherent" state (e.g., handling contacts, available for contacts, authorized project work) divided by their total scheduled productive time. It specifically excludes non-productive time such as breaks, lunches, or meetings unless they are part of a planned, adherent activity.
          business_purpose: >
            The primary purpose of measuring adherence is to ensure optimal resource allocation and operational efficiency. It helps managers verify that staffing levels align with forecasted contact volumes, thereby minimizing customer wait times and controlling labor costs. Consistently low adherence can indicate issues with agent engagement, time management, or systemic problems that prevent agents from following their schedule, prompting targeted coaching or process improvements.
          calculation_logic: "SUM(Total seconds in adherent states) / SUM(Total seconds in scheduled productive states)"
          calculation_example: >
            Scenario: An agent is scheduled for 8 hours, with 1 hour of unpaid breaks.
            Scheduled Productive Time: 7 hours = 25,200 seconds.
            Actual Adherent Time: The agent spent 6.5 hours (23,400 seconds) in adherent states.
            Calculation: 23,400 / 25,200 = 0.9286
            Result: The agent's adherence for the day is 92.86%.
          data_source_details:
            model: dim_agent_scheduling_daily
            columns:
              - adherence_productive_time (numerator, in seconds)
              - scheduled_productive_time (used in denominator metric, in seconds)
          units: "Ratio (0.00 to 1.00), typically presented as a percentage (e.g., 92.86%)."
          interpretation_notes: >
            A high adherence rate (typically >90-95%) is desirable.
            100% adherence is often unrealistic and can be a negative sign, suggesting agents are not taking necessary micro-breaks, which could lead to burnout.
            Low adherence requires investigation to understand the root cause (e.g., technical issues, excessive non-adherent but necessary work, behavioral issues).

      - name: scheduled_productive_hours
        label: Productive Hours
        model: ref('dim_cs_agent_scorecard')
        description: Total scheduled productive hours for agents
        type: sum
        sql: (1.0/(60*60)) * scheduled_productive_time
        timestamp: summary_date
        time_grains: [day, week, month, quarter, year]
        dimensions:
          - agent
          - team
        meta:
          domain: cs
          category: agent_performance
          definition_long: >
            This metric represents the total number of hours an agent is scheduled to be engaged in productive, customer-facing, or directly value-adding activities. It forms the baseline for what is expected from an agent during their shift and is a fundamental component for calculating other efficiency metrics. It explicitly excludes scheduled non-productive time like meal breaks or unpaid time off.
          business_purpose: >
            Scheduled Productive Hours is a foundational metric for workforce management and capacity planning. It is the denominator for critical efficiency ratios like Adherence and Occupancy. Tracking this allows the business to understand planned capacity, forecast staffing needs, and ensure that scheduling aligns with strategic operational goals.
          calculation_logic: "SUM(scheduled_productive_time_in_seconds) / 3600"
          calculation_example: >
            Scenario: A team of 10 agents each works a standard 8-hour shift with 1 hour of breaks.
            Individual Productive Time: Each agent has 7 scheduled productive hours (25,200 seconds).
            Team Calculation: (10 agents * 25,200 seconds/agent) / 3600 seconds/hour = 70 hours.
            Result: The team has 70 Scheduled Productive Hours for the day.
          data_source_details:
            model: dim_cs_agent_scorecard
            columns:
              - scheduled_productive_time (in seconds)
          units: "Hours"
          interpretation_notes: >
            This is a volume metric, not a performance indicator on its own. It provides the context of scale for other metrics. A change in this value typically reflects changes in staffing or scheduling strategy.

      - name: occupancy
        label: Occupancy Rate
        model: ref('dim_cs_agent_scorecard')
        description: Proportion of time spent in occupancy state relative to productive scheduled time
        type: ratio
        sql: sum(1.0000 * occupancy_productive_time) / nullif(sum(scheduled_productive_time), 0)
        timestamp: summary_date
        time_grains: [day, week, month, quarter, year]
        dimensions:
          - agent
          - team
        meta:
          domain: cs
          category: agent_performance
          definition_long: >
            Occupancy measures the proportion of an agent's logged-in, productive time that is spent actively handling customer interactions (e.g., talk time, hold time, wrap-up time). It is a key indicator of agent utilization and workload, showing how "busy" agents are with handling contacts versus being idle or available.
          business_purpose: >
            This metric is critical for assessing workforce efficiency and preventing both agent burnout and underutilization. A very high occupancy rate (e.g., >95%) over a sustained period can signal understaffing and lead to burnout. A low rate may indicate overstaffing, low contact volume, or operational inefficiencies, presenting an opportunity to reallocate resources or provide agents with other tasks.
          calculation_logic: "SUM(Total time spent handling contacts in seconds) / SUM(Total scheduled productive time in seconds)"
          calculation_example: >
            Scenario: An agent has 7 scheduled productive hours (25,200 seconds).
            Time Spent on Contacts: The agent spends 4 hours on calls, 1 hour on hold, and 1.25 hours in after-call work. Total occupancy time = 6.25 hours (22,500 seconds).
            Calculation: 22,500 / 25,200 = 0.8928
            Result: The agent's occupancy for the day is 89.28%.
          data_source_details:
            model: dim_cs_agent_scorecard
            columns:
              - occupancy_productive_time (numerator, in seconds)
              - scheduled_productive_time (denominator, in seconds)
          units: "Ratio (0.00 to 1.00), typically presented as a percentage (e.g., 89.28%)."
          interpretation_notes: >
            The ideal occupancy rate is typically between 85% and 90%.
            Consistently high occupancy (>90%) is a significant risk factor for employee attrition.
            Consistently low occupancy (<80%) suggests a need to review staffing models or forecast accuracy.

      - name: first_contact_resolution_rate
        label: First Contact Resolution Rate
        model: ref('dim_ticket_summary_slim')
        description: Percentage of tickets resolved on first contact without requiring follow-ups
        type: ratio
        sql: >
          sum(case when resolution_contacts = 1 and status in ('solved', 'closed') then 1 else 0 end) / nullif(sum(case when status in ('solved', 'closed', 'open', 'pending') then 1 else 0 end), 0)
        timestamp: ticket_created_at
        time_grains: [day, week, month, quarter, year]
        dimensions:
          - ticket
          - agent
          - team
          - client
        meta:
          domain: cs
          category: performance
          definition_long: >
            First Contact Resolution (FCR), also known as First Call Resolution, measures the percentage of customer inquiries that are successfully resolved during the initial interaction. This means the customer does not need to call back, email again, or have the issue escalated to resolve their problem. It is a powerful indicator of both customer satisfaction and operational efficiency.
          business_purpose: >
            A high FCR rate is strongly correlated with increased customer satisfaction and loyalty, as it reflects an effortless customer experience. Operationally, it reduces the costs associated with repeat contacts and frees up agent capacity to handle new, incoming issues. Tracking FCR helps identify knowledge gaps, process inefficiencies, or empowerment issues within the support team.
          calculation_logic: "(Count of tickets resolved in a single contact) / (Total count of resolvable tickets created in the period)"
          calculation_example: >
            Scenario: In one week, the support team receives 500 new, resolvable tickets.
            Resolved on First Contact: Analysis of these tickets shows that 375 of them were marked as 'solved' after only one interaction (resolution_contacts = 1).
            Total Resolvable Tickets: The total number of tickets created in the period that are eligible for resolution is 500.
            Calculation: 375 / 500 = 0.75
            Result: The FCR rate for the week is 75%.
          data_source_details:
            model: dim_ticket_summary_slim
            columns:
              - ticket_id
              - resolution_contacts
              - status
              - ticket_created_at
          units: "Ratio (0.00 to 1.00), presented as a percentage (e.g., 75%)."
          interpretation_notes: >
            Industry benchmarks for FCR vary by channel (e.g., phone vs. email) but are typically in the 70-80% range.
            This metric can be susceptible to 'gaming' if not properly audited. It should be analyzed alongside Customer Satisfaction (CSAT) scores to ensure high FCR isn't coming at the expense of a quality resolution.
            The definition of 'contact' and 'resolution' must be standardized across the organization.

      - name: chat_acceptance_rate
        label: Chat Acceptance Rate
        model: ref('dim_cs_agent_scorecard')
        description: Rate of chats accepted relative to total chats assigned
        type: ratio
        sql: sum(1.0000 * accepted_pings_chat) / nullif(sum(1.0000 * assigned_and_accepted_pings_chat), 0)
        timestamp: summary_date
        time_grains: [day, week, month, quarter, year]
        dimensions:
          - agent
          - team
        meta:
          domain: cs
          category: chat_performance
          definition_long: >
            The Chat Acceptance Rate is the percentage of incoming chat requests routed to an agent that are actually accepted by that agent before they time out or are reassigned. It is a direct measure of an agent's attentiveness and availability to engage with customers via the chat channel.
          business_purpose: >
            This metric is crucial for managing the real-time customer experience in the chat queue. A low acceptance rate leads to longer customer wait times and higher abandonment rates. It helps supervisors identify agents who may be avoiding work, are overwhelmed with their current tasks (and thus cannot accept more), or are experiencing technical difficulties with their chat client.
          calculation_logic: "SUM(Total chats accepted by an agent) / SUM(Total chats assigned to that agent)"
          calculation_example: >
            Scenario: An agent is assigned 50 incoming chat pings during their shift.
            Accepted Chats: The agent clicks 'accept' on 48 of those chats.
            Calculation: 48 / 50 = 0.96
            Result: The agent's Chat Acceptance Rate is 96%.
          data_source_details:
            model: dim_cs_agent_scorecard
            columns:
              - accepted_pings_chat (numerator)
              - assigned_and_accepted_pings_chat (denominator, representing total assigned)
          units: "Ratio (0.00 to 1.00), presented as a percentage (e.g., 96%)."
          interpretation_notes: >
            A high rate (typically >95%) is desirable.
            A low rate warrants immediate investigation. It can indicate behavioral issues, system problems, or an agent being overloaded with concurrent work.

      - name: chat_readiness_rate
        label: Chat Readiness Rate
        model: ref('dim_cs_agent_scorecard')
        description: Proportion of time spent in chat ready state relative to scheduled chat time
        type: ratio
        sql: (1.0000 * sum(chat_online_time)) / nullif(sum(scheduled_time_chat), 0)
        timestamp: summary_date
        time_grains: [day, week, month, quarter, year]
        dimensions:
          - agent
          - team
        meta:
          domain: cs
          category: chat_performance
          definition_long: >
            The Chat Readiness Rate measures the percentage of time an agent was in an 'Available' or 'Ready' status for the chat channel, compared to the total time they were scheduled to be available for chats. It is a channel-specific version of schedule adherence.
          business_purpose: >
            This metric ensures that staffing for the chat channel is effective and that agents are prepared to receive customer inquiries as planned. It helps managers diagnose capacity issues, differentiating between problems of overall adherence versus issues specific to the chat channel (e.g., an agent forgetting to set their status to 'available' after handling an email).
          calculation_logic: "SUM(Total time in 'Ready' state for chat) / SUM(Total time scheduled for chat)"
          calculation_example: >
            Scenario: An agent is scheduled for 4 hours of dedicated chat time (14,400 seconds).
            Actual Ready Time: Due to other tasks, the agent was only in a 'Ready' status for 3.8 hours (13,680 seconds).
            Calculation: 13,680 / 14,400 = 0.95
            Result: The agent's Chat Readiness Rate is 95%.
          data_source_details:
            model: dim_cs_agent_scorecard
            columns:
              - chat_online_time (numerator, in seconds)
              - scheduled_time_chat (denominator, in seconds)
          units: "Ratio (0.00 to 1.00), presented as a percentage (e.g., 95%)."
          interpretation_notes: >
            Similar to Adherence, a high rate (>95%) is expected.
            Low rates can indicate poor time management, technical issues, or flawed scheduling.

      - name: missed_chats
        label: Missed Chats
        model: ref('dim_cs_agent_scorecard')
        description: Count of chat pings that were not accepted
        type: sum
        sql: assigned_pings_chat - accepted_pings_chat
        timestamp: summary_date
        time_grains: [day, week, month, quarter, year]
        dimensions:
          - agent
          - team
        meta:
          domain: cs
          category: chat_performance
          definition_long: >
            Missed Chats is a count of the number of chat requests that were assigned to an agent but were not accepted, resulting in the customer waiting longer or abandoning the request. It is the absolute number counterpart to the Chat Acceptance Rate.
          business_purpose: >
            This metric directly quantifies lost opportunities for customer engagement and highlights service level failures. A high number of missed chats indicates a poor customer experience and potential inefficiencies in the chat routing or staffing model. It helps managers pinpoint specific agents or times of day where issues are most prevalent.
          calculation_logic: "SUM(Total chats assigned) - SUM(Total chats accepted)"
          calculation_example: >
            Scenario: An agent was assigned 50 chats during their shift and accepted 48 of them.
            Calculation: 50 - 48 = 2
            Result: The agent had 2 Missed Chats.
          data_source_details:
            model: dim_cs_agent_scorecard
            columns:
              - assigned_pings_chat (total assigned)
              - accepted_pings_chat (total accepted)
          units: "Count of chats"
          interpretation_notes: >
            The ideal value is 0.
            Any number greater than zero represents a direct service failure and should be investigated.
            This metric should be monitored in real-time if possible to allow for immediate intervention.

      - name: phone_readiness
        label: Phone Readiness Rate
        model: ref('dim_cs_agent_scorecard')
        description: Proportion of time spent in any state other than phone not ready state relative to scheduled phone time
        type: ratio
        sql: 1 - ((1.0000 * sum(notready_time)) / nullif(sum(scheduled_time_phone), 0))
        timestamp: summary_date
        time_grains: [day, week, month, quarter, year]
        dimensions:
          - agent
          - team
        meta:
          domain: cs
          category: phone_performance
          definition_long: >
            Phone Readiness measures the percentage of an agent's scheduled phone time that they are actually available to take calls. It is calculated by taking the total scheduled phone time and subtracting any time spent in a 'Not Ready' status.
          business_purpose: >
            This KPI is vital for ensuring the phone queue is adequately staffed. It helps managers understand if agents are available as scheduled or if they are spending excessive time in states that prevent them from receiving calls. It's a key lever for controlling service levels and customer wait times.
          calculation_logic: "1 - (SUM(Time in 'Not Ready' status) / SUM(Total time scheduled for phone))"
          calculation_example: >
            Scenario: An agent is scheduled for 4 hours of phone time (14,400 seconds).
            Not Ready Time: The agent spent 30 minutes (1,800 seconds) in a 'Not Ready' state.
            Calculation: 1 - (1800 / 14400) = 1 - 0.125 = 0.875
            Result: The agent's Phone Readiness was 87.5%.
          data_source_details:
            model: dim_cs_agent_scorecard
            columns:
              - notready_time (in seconds)
              - scheduled_time_phone (in seconds)
          units: "Ratio (0.00 to 1.00), presented as a percentage (e.g., 87.5%)."
          interpretation_notes: >
            A high rate is desirable. Low readiness directly impacts the call center's ability to answer calls promptly.
            Reasons for 'Not Ready' time should be categorized and analyzed to see if they are legitimate (e.g., post-call work, coaching) or problematic.

      - name: on_call_rate
        label: On Call Rate
        model: ref('dim_cs_agent_scorecard')
        description: Proportion of time spent in on call state relative to productive scheduled phone time
        type: ratio
        sql: (1.0000 * sum(on_call_time)) / nullif(sum(scheduled_time_phone), 0)
        timestamp: summary_date
        time_grains: [day, week, month, quarter, year]
        dimensions:
          - agent
          - team
        meta:
          domain: cs
          category: phone_performance
          definition_long: >
            On Call Rate is a channel-specific occupancy metric. It measures the percentage of an agent's scheduled phone time that is spent actively engaged with a customer on a call (including talk time and hold time).
          business_purpose: >
            This metric indicates how utilized agents are specifically within the phone channel. It helps managers gauge workload and efficiency for phone-dedicated staff. It can help determine if the phone team is over- or under-staffed relative to the incoming call volume.
          calculation_logic: "SUM(Total time on call) / SUM(Total time scheduled for phone)"
          calculation_example: >
            Scenario: An agent is scheduled for 4 hours of phone time (14,400 seconds).
            On Call Time: The agent spent a total of 3 hours (10,800 seconds) on calls (talking or holding).
            Calculation: 10800 / 14400 = 0.75
            Result: The agent's On Call Rate was 75%.
          data_source_details:
            model: dim_cs_agent_scorecard
            columns:
              - on_call_time (numerator, in seconds)
              - scheduled_time_phone (denominator, in seconds)
          units: "Ratio (0.00 to 1.00), presented as a percentage (e.g., 75%)."
          interpretation_notes: >
            Unlike readiness, a very high On Call Rate can be a negative sign of burnout, similar to overall Occupancy.
            A rate that is too low suggests overstaffing or low call volume.

      - name: avg_csat_score
        label: Average CSAT Score
        model: ref('dim_ticket_summary_slim')
        description: Average customer satisfaction score received
        type: average
        sql: csat_score_given
        timestamp: last_solved_date
        time_grains: [day, week, month, quarter, year]
        dimensions:
          - ticket
          - agent
          - team
          - client
        meta:
          domain: cs
          category: satisfaction
          definition_long: >
            The Average Customer Satisfaction (CSAT) Score is a transactional metric that represents the mean satisfaction level reported by customers following a specific interaction or ticket resolution. It is typically captured via a post-interaction survey asking a question like, "How satisfied were you with the support you received?" with responses on a numerical scale (e.g., 1-5 or 1-10).
          business_purpose: >
            CSAT is a direct measure of the quality of customer service from the customer's perspective. It provides immediate feedback on agent performance, product issues, and process effectiveness. Tracking the average score helps leadership gauge overall service quality, identify high-performing agents and teams for recognition, and pinpoint areas or individuals requiring coaching and improvement. Trends in CSAT can be leading indicators of customer churn or loyalty.
          calculation_logic: "SUM(All CSAT scores received) / COUNT(Total number of CSAT responses received)"
          calculation_example: >
            Scenario: Over one day, the team receives 5 CSAT survey responses for solved tickets.
            Scores Received: The scores are 5, 4, 5, 3, and 5.
            Calculation: (5 + 4 + 5 + 3 + 5) / 5 = 22 / 5 = 4.4
            Result: The Average CSAT Score for the day is 4.4 out of 5.
          data_source_details:
            model: dim_ticket_summary_slim
            columns:
              - csat_score_given
              - last_solved_date
          units: "Score on a defined scale (e.g., a 4.4 on a 1-5 scale)."
          interpretation_notes: >
            A higher score is universally better. The target value depends on the scale used (e.g., >4.2 on a 5-point scale is often considered good).
            It's crucial to also track the response rate (CSAT Coverage), as a high score with a very low response rate may not be representative of the entire customer base.
            This metric reflects a specific interaction, not overall brand loyalty (which is better measured by NPS).

      - name: csat_responses
        label: CSAT Responses
        model: ref('dim_ticket_summary_slim')
        description: Total number of CSAT responses received
        type: count
        sql: case when csat_score_given is not null then ticket_id else null end
        timestamp: last_solved_date
        time_grains: [day, week, month, quarter, year]
        dimensions:
          - ticket
          - agent
          - team
          - client
        meta:
          domain: cs
          category: satisfaction
          definition_long: >
            This metric is a simple count of the total number of customer satisfaction surveys that have been completed and returned by customers within a given time period.
          business_purpose: >
            CSAT Responses provides the sample size for all other CSAT-related metrics (like Average Score and CSAT 5 Rate). It's the numerator for calculating the CSAT Coverage Rate. A higher number of responses increases the statistical significance and reliability of the satisfaction data.
          calculation_logic: "COUNT(tickets where csat_score_given is not null)"
          calculation_example: >
            Scenario: 500 CSAT surveys were sent out to customers in a week.
            Completed Surveys: 120 customers filled out and submitted the survey.
            Result: The number of CSAT Responses is 120.
          data_source_details:
            model: dim_ticket_summary_slim
            columns:
              - csat_score_given
              - ticket_id
          units: "Count of responses"
          interpretation_notes: >
            This is a volume metric. While higher is generally better for data quality, the primary goal is to ensure the response rate is sufficient for representative analysis.

      - name: csat_5_rate
        label: CSAT 5 Rate
        model: ref('dim_ticket_summary_slim')
        description: Proportion of CSAT scores that are 5 (excellent)
        type: ratio
        sql: {{ metric('csat_score_5') }} / nullif({{ metric('csat_responses') }}, 0)
        timestamp: last_solved_date
        time_grains: [day, week, month, quarter, year]
        dimensions:
          - ticket
          - agent
          - team
          - client
        meta:
          domain: cs
          category: satisfaction
          definition_long: >
            The CSAT 5 Rate, or "Top Box Rate," is the percentage of all customer satisfaction responses that received the highest possible score (e.g., 5 out of 5).
          business_purpose: >
            This metric measures the consistency of delivering excellent service, going beyond the average score. A high CSAT 5 Rate indicates that a large proportion of customer interactions are not just satisfactory, but exceptional. It's a powerful indicator of high-quality service and can be a more sensitive measure of performance than the overall average.
          calculation_logic: "Total Count of CSAT Score 5 / Total Count of CSAT Responses"
          calculation_example: >
            Scenario: The team received 75 '5-star' ratings from a total of 120 survey responses.
            Calculation: 75 / 120 = 0.625
            Result: The CSAT 5 Rate is 62.5%.
          data_source_details:
            model: dim_ticket_summary_slim (via derived metrics)
            columns:
              - N/A (Derived from csat_score_5 and csat_responses)
          units: "Ratio (0.00 to 1.00), presented as a percentage (e.g., 62.5%)."
          interpretation_notes: >
            Higher is always better. It signifies a team's ability to consistently delight customers.
            A team could have a good average CSAT score but a low CSAT 5 Rate, indicating performance is consistently "good" but rarely "great."

      - name: csat_coverage_rate
        label: CSAT Coverage Rate
        model: ref('dim_ticket_summary_slim')
        description: Percentage of all solved tickets with a CSAT response
        type: ratio
        sql: {{ metric('csat_responses') }} / nullif({{ metric('tickets_solved') }}, 0)
        timestamp: last_solved_date
        time_grains: [day, week, month, quarter, year]
        dimensions:
          - ticket
          - agent
          - team
          - client
        meta:
          domain: cs
          category: satisfaction
          definition_long: >
            The CSAT Coverage Rate measures the percentage of all resolved customer support tickets for which a CSAT survey response was received. It essentially calculates the survey response rate.
          business_purpose: >
            This metric is crucial for understanding the statistical validity of all CSAT-related data. A low coverage rate means that satisfaction scores are based on a small, potentially biased sample of customers, making the results less representative of the true overall customer experience. Tracking this helps diagnose issues with survey delivery or customer survey fatigue.
          calculation_logic: "Total Count of CSAT Responses / Total Count of Solved Tickets"
          calculation_example: >
            Scenario: In one month, the team solved 10,000 tickets and received 1,500 CSAT survey responses.
            Calculation: 1,500 / 10,000 = 0.15
            Result: The CSAT Coverage Rate is 15%.
          data_source_details:
            model: dim_ticket_summary_slim (via derived metrics)
            columns:
              - N/A (Derived from csat_responses and tickets_solved)
          units: "Ratio (0.00 to 1.00), presented as a percentage (e.g., 15%)."
          interpretation_notes: >
            There's no universal "good" rate, but higher is better for data confidence. A rate below 5-10% might call the validity of CSAT scores into question.
            A sudden drop can indicate technical problems with the survey distribution system.

      - name: tickets_solved
        label: Solved Tickets
        model: ref('dim_ticket_summary_slim')
        description: Total number of tickets solved
        type: count
        sql: case when status in ('solved', 'closed') then ticket_id else null end
        timestamp: last_solved_date
        time_grains: [day, week, month, quarter, year]
        dimensions:
          - ticket
          - agent
          - team
          - client
        meta:
          domain: cs
          category: tickets
          definition_long: >
            This is a count of the total number of customer support tickets that were moved into a terminal 'Solved' or 'Closed' status during a given period. It represents the team's total output.
          business_purpose: >
            Tickets Solved is a primary measure of team productivity and throughput. It helps managers understand workload capacity and performance against goals. It also serves as a critical denominator for other metrics, such as the CSAT Coverage Rate.
          calculation_logic: "COUNT(DISTINCT tickets where status is 'solved' or 'closed')"
          calculation_example: >
            Scenario: We are measuring the output of Team A for the week.
            Data: The team moved 520 unique tickets to a 'solved' state.
            Result: The number of Tickets Solved is 520.
          data_source_details:
            model: dim_ticket_summary_slim
            columns:
              - ticket_id
              - status
              - last_solved_date
          units: "Count of tickets"
          interpretation_notes: >
            A volume metric that reflects team output. More is generally better, but it must be balanced with quality metrics like CSAT and FCR to ensure agents are not rushing to close tickets improperly.

      - name: avg_qa_score
        label: Average QA Score
        model: ref('prep_cx_level_ai_qa_scores')
        description: Average quality assurance score received
        type: average
        sql: avg_qa_score
        timestamp: last_solved_date
        time_grains: [day, week, month, quarter, year]
        dimensions:
          - interaction
          - ticket
          - agent
          - team
          - client
        meta:
          domain: cs
          category: quality
          definition_long: >
            The Average Quality Assurance (QA) Score is the average score agents receive from internal quality reviewers. These reviews are typically conducted by a dedicated QA team or managers who grade interactions against a scorecard of criteria, such as procedural adherence, accuracy of information, and soft skills.
          business_purpose: >
            QA scores provide an internal, standardized measure of service quality and process compliance. Unlike CSAT, which measures customer perception, QA measures adherence to business standards. It is a vital tool for agent coaching, identifying training needs, and ensuring a consistent and high-quality customer experience.
          calculation_logic: "SUM(All individual QA scores) / COUNT(Total number of QA evaluations)"
          calculation_example: >
            Scenario: An agent had two interactions evaluated in a month.
            Scores: Evaluation 1 received a score of 95%. Evaluation 2 received a score of 88%.
            Calculation: (95 + 88) / 2 = 91.5
            Result: The agent's Average QA Score is 91.5%.
          data_source_details:
            model: prep_cx_level_ai_qa_scores
            columns:
              - avg_qa_score
          units: "Score (typically a percentage from 0-100)"
          interpretation_notes: >
            Higher is always better, indicating strong alignment with company quality standards.
            It should be analyzed alongside CSAT to ensure that internal standards align with what actually satisfies customers.
      
      # =============================================================================
      # TRANSFERS DOMAIN METRICS
      # =============================================================================
      - name: account_value
        label: Account Value
        model: ref('money_movement__dim_accounts')
        description: >
          Value of the client's account involved in the transfer, indicating business importance and risk level.
        type: sum
        sql: account_value
        timestamp: transitioned_date
        time_grains: [day, week, month, quarter, year]
        dimensions:
          - transfer
        meta:
          domain: transfers
          category: financial
          definition_long: >
            Account Value represents the total monetary value of the client's account that is being moved in a transfer operation. This metric can be aggregated to show the total value of assets in motion during a specific period.
          business_purpose: >
            This metric is a key indicator of the financial significance of transfer operations. It is used for risk assessment, prioritization of high-value transfers, and reporting to leadership on the scale of business being handled. It helps to contextualize the importance of the transfer process to the company's bottom line.
          calculation_logic: "SUM(account_value)"
          calculation_example: >
            Scenario: Three transfers are processed in one day.
            Values: Transfer A is for an account worth $1,000,000. Transfer B is for an account worth $2,500,000. Transfer C is for an account worth $500,000.
            Calculation: 1,000,000 + 2,500,000 + 500,000 = $4,000,000
            Result: The total Account Value transferred that day is $4,000,000.
          data_source_details:
            model: money_movement__dim_accounts
            columns:
              - account_value
          units: "Currency (USD)"
          interpretation_notes: >
            A volume-based financial metric. It's not a performance indicator on its own but provides critical business context for other performance metrics like Transfer Duration.

      - name: transfer_duration_days
        label: Transfer Duration (Days)
        model: ref('money_movement__fct_institutional_transfer_state_histories')
        description: >
          Total duration of the full transfer lifecycle from first state to terminal state.
        type: average
        sql: datediff('days', transfer_created_at, cal_mapping_end_date::date)
        timestamp: transitioned_date
        time_grains: [day, week, month, quarter, year]
        dimensions:
          - transfer
        meta:
          domain: transfers
          category: performance
          definition_long: >
            Transfer Duration is a performance metric that calculates the average elapsed time, in days, for a financial transfer to complete its entire lifecycle. The duration is measured from the moment the transfer process is initiated (creation date) to the moment it reaches a final, terminal state (e.g., 'Completed', 'Cancelled', 'Failed').
          business_purpose: >
            This metric is a critical indicator of the efficiency and speed of the transfer process. A shorter duration generally leads to higher customer satisfaction, reduced operational overhead, and lower financial risk. Monitoring this KPI helps the business identify bottlenecks in the transfer workflow, assess the impact of process changes, and set realistic service level agreements (SLAs) with clients.
          calculation_logic: "For each completed transfer, calculate: DATEDIFF('day', creation_timestamp, terminal_state_timestamp). The final metric is the AVERAGE of these individual durations over a given period."
          calculation_example: >
            Scenario: We are analyzing three transfers that completed in the last month.
            Transfer A: Created Jan 1, Completed Jan 10. Duration = 9 days.
            Transfer B: Created Jan 5, Completed Jan 9. Duration = 4 days.
            Transfer C: Created Jan 8, Completed Jan 20. Duration = 12 days.
            Calculation: (9 + 4 + 12) / 3 = 25 / 3 = 8.33
            Result: The average transfer duration for the period is 8.33 days.
          data_source_details:
            model: money_movement__fct_institutional_transfer_state_histories
            columns:
              - transfer_created_at (start of duration)
              - cal_mapping_end_date (end of duration for a terminal state)
          units: "Days"
          interpretation_notes: >
            A lower number is universally better, indicating a faster and more efficient process.
            It's important to segment this metric by transfer type, contra-firm, or value, as different transfer types may have inherently different expected durations.
            Outliers with extremely long durations should be investigated individually as they may represent significant process failures or complex edge cases.

      - name: sla_breach_count
        label: SLA Breaches
        model: ref('money_movement__fct_institutional_transfer_state_histories')
        description: Count of transfers that have breached SLA thresholds
        type: count
        sql: case when sla_flag = 1 then transfer_id else null end
        timestamp: transitioned_date
        time_grains: [day, week, month, quarter, year]
        dimensions:
          - transfer
        meta:
          domain: transfers
          category: sla
          definition_long: >
            This metric provides a raw count of the number of individual transfers that have failed to meet their predefined Service Level Agreement (SLA) for overall completion time. An SLA breach occurs when the total duration of a transfer exceeds the contractually or internally agreed-upon timeframe.
          business_purpose: >
            Tracking SLA breaches is fundamental to risk management, client relationship management, and regulatory compliance. It provides a clear, unambiguous signal of service failures. A high or rising number of breaches necessitates immediate investigation to identify root causes, which could range from partner delays to internal process inefficiencies. This metric is often reported to leadership and clients as a key measure of service reliability.
          calculation_logic: "COUNT(Distinct transfer_ids WHERE the sla_flag is marked as true or 1)"
          calculation_example: >
            Scenario: In a given week, 1,000 transfers are processed.
            SLA Breach Identification: The data pipeline analyzes each transfer's duration against its specific SLA. It flags 15 unique transfers as having exceeded their SLA.
            Calculation: COUNT(transfers where sla_flag = 1) = 15
            Result: The SLA Breach Count for the week is 15.
          data_source_details:
            model: money_movement__fct_institutional_transfer_state_histories
            columns:
              - sla_flag (the boolean or integer indicating a breach)
              - transfer_id (the entity being counted)
          units: "Count of transfers"
          interpretation_notes: >
            The goal is always to have this number be as close to zero as possible.
            This count should be viewed in context of the total transfer volume (i.e., as a percentage) to understand its relative severity.
            Analysis should be segmented by transfer type, partner, and value to pinpoint the source of the breaches.

